using Parameters
using Flux; import Flux
using Game2048: bitboard_to_array, Dirs
using CommonRLInterface; const rli = CommonRLInterface;
using AlphaZero; const ann = AlphaZero.Network
include("game.jl")

"""
MCTS integrated with neural network. Search mcts(s) returns probability distribution
with value evaluation. 
"""
@with_kw mutable struct MonteCarloTreeSearchNN
    env # problem
    net<:ann.AbstractNetwork # network

    N_sa::Dict{Tuple, Int} = Dict() # visit counts for (s,a)
    Q::Dict{Tuple, Float64} = Dict() # action values
    P = Dict() # Policy distribution over actions from net
    Outcomes = Dict()

    d::Int # depth
    m::Int # number of simulations
    c::Float64 # exploration constant
end

function (œÄ::MonteCarloTreeSearchNN)(s, œÑ::Float64 = 1.0)::Vector{Float64}
    @unpack env, m, N_sa = œÄ
    for _ in m
        search!(œÄ, s, curr_step, max_step, œÄ.d)
    end

    # Counts provide a good estimation of the improved policy (via UCB maximization)
    ùíú = rli.actions(env)
    counts = [(haskey(N_sa, (s,a)) ? N_sa[(s,a)] : 0) for a in ùíú]

    if œÑ == 0 # greedy action selection
        best_action_idx = rand(finall(x->x==maximum(counts), counts))
        probs = zeros(length(ùíú))
        probs[best_action_idx] .= 1
        return probs
    end

    counts = [count ^ (1.0/temp) for count in counts]
    denom = sum(counts)
    probs = counts ./ denom
    return probs
end

function search!(œÄ::MonteCarloTreeSearchNN, s, curr_step, max_step, d)
    @unpack N_sa, Q, P, Outcomes = œÄ
    @unpack d, m, c = œÄ
    @unpack env, net = œÄ
    @unpack goal, Œ≥ = env

    if !haskey(Outcomes, s)
        Outcomes[s] = rli.reward(s, goal, curr_step, max_step)
    end
    if Outcomes[s] != 0.0 # Backup on terminal state
        return Outcomes[s]
    end
    if d ‚â§ 0 # Backup on horizon depth state
        p, v = net(s)
        return v #! insert neural network value prediction here
    end

    if !haskey(P, s) # Expansion on leaf node state
        p, v = net(s) #! insert neural network predictions here
        ùíú_size = length(rli.actions(env))
        valid_mask = valid_action_mask(s, ùíú_size)
        p .*= valid_mask
        sum_p = sum(p)

        if sum_p > 0 # renormalize for distribution over valid actions
            p ./= sum_p
        else # if no valid actions, uniform distribution
            p .= 1/ùíú_size 
        end

        P[s] = p
        for a in ùíú_size
            N_sa[(s,a)] = 0
            Q[(s,a)] = 0.0
        end

        return v
    end

    best_action = selection(s, rli.actions(env), N_sa, Q, P, c)
    s‚Ä≤, r, terminated = transition(s, best_action, goal, curr_step, max_step)
    if terminated
        return r
    end

    v = search!(œÄ, s‚Ä≤, curr_step+1, max_step, d-1)

    Q[(s,best_action)] = (N_sa[(s,best_action)] * Q[(s, best_action)] + v) / (N_sa[(s,best_action)] + 1)
    N_sa[(s,best_action)] += 1

    return v
end

function selection(s, ùíú, N_sa, Q, P, c::Float64)::Int # UCT 
    N_s = sum(N[(s,a)] for a in ùíú)
    N_s = (N_s == 0) ? Inf : N_s

    ùíú_valid = valid_actions(s)
    uct(s,a) = Q[(s,a)] + c * P[s][a] * sqrt(N_s) / (1 + N_sa[(s,a)])
    best_action = argmax(Dict(
        a => uct(s,a) for a in ùíú_valid
    ))
    return best_action
end