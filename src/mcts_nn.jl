using Parameters
using Flux; import Flux
using Game2048: bitboard_to_array, Dirs
using CommonRLInterface; const rli = CommonRLInterface;
using AlphaZero; 
include("game.jl")

"""
MCTS integrated with neural network. Search mcts(s) returns probability distribution
with value evaluation. 
"""
@with_kw mutable struct MonteCarloTreeSearchNN
    env # problem
    net # network

    N_sa::Dict{Tuple, Int} = Dict() # visit counts for (s,a)
    Q::Dict{Tuple, Float64} = Dict() # action values
    P = Dict() # Policy distribution over actions from net
    Outcomes = Dict()

    d::Int # depth
    m::Int # number of simulations
    c::Float64 # exploration constant
end

function (Ï€::MonteCarloTreeSearchNN)(s; Ï„::Float64 = 1.0)::Vector{Float64}
    @unpack env, m = Ï€
    for _ in 1:m
        search!(Ï€, s, env.curr_step, env.max_step, Ï€.d)
    end

    # Counts provide a good estimation of the improved policy (via UCB maximization)
    ğ’œ = rli.actions(env)
    counts = [(haskey(Ï€.N_sa, (s,a)) ? Ï€.N_sa[(s,a)] : 0) for a in ğ’œ]

    if Ï„ == 0 # greedy action selection
        best_action_idx::Int = rand(findall(x->x==maximum(counts), counts))
        probs = zeros(length(ğ’œ))
        probs[best_action_idx] = 1.0
        return probs
    end

    counts = [count ^ (1.0/Ï„) for count in counts]
    denom = sum(counts)
    # probs = (denom==0.0) ? 0.25 * ones(4) : counts ./ denom
    probs = counts ./ denom
    return probs
end

function search!(Ï€::MonteCarloTreeSearchNN, s, curr_step::Int, max_step::Int, d::Int)::AbstractFloat
    @unpack env, net = Ï€
    @unpack T, R, terminated, goal, Î³ = env

    net = net |> cpu

    if d â‰¤ 0 # Backup on horizon depth state
        p, v = only(net(unsqueeze_batch_maybe(s)))
        return only(v)
    end
    if !haskey(Ï€.Outcomes, s)
        Ï€.Outcomes[s] = R(s, goal, curr_step, max_step)
    end
    if terminated(s, goal, curr_step, max_step) # Backup on terminal state
        return Ï€.Outcomes[s]
    end


    if !haskey(Ï€.P, s) # Expansion on leaf node state
        p, v = only(net(unsqueeze_batch_maybe(s)))
        ğ’œ = rli.actions(env)
        valid_mask = valid_action_mask(s, length(ğ’œ))
        p .*= valid_mask
        sum_p = sum(p)

        if sum_p > 0 # renormalize for distribution over valid actions
            p ./= sum_p
        else # if no valid actions, uniform distribution
            p .= 1/length(ğ’œ)
        end

        Ï€.P[s] = p
        for a in ğ’œ
            Ï€.N_sa[(s,a)] = 0
            Ï€.Q[(s,a)] = 0.0
        end

        return only(v)
    end

    best_action = selection(s, rli.actions(env), Ï€.N_sa, Ï€.Q, Ï€.P, Ï€.c)
    sâ€², _, r, done = T(s, best_action, goal, curr_step, max_step)

    v = search!(Ï€, sâ€², curr_step+1, max_step, d-1)

    Ï€.Q[(s,best_action)] = (Ï€.N_sa[(s,best_action)] * Ï€.Q[(s, best_action)] + v) / (Ï€.N_sa[(s,best_action)] + 1)
    Ï€.N_sa[(s,best_action)] += 1

    return v
end

function selection(s, ğ’œ, N_sa, Q, P, c::Float64)::Int # UCT 
    N_s = sum(N_sa[(s,a)] for a in ğ’œ)
    N_s = (N_s == 0) ? Inf : N_s

    ğ’œ_valid = valid_actions(s)
    uct(s,a) = Q[(s,a)] + c * P[s][a+1] * sqrt(N_s) / (1 + N_sa[(s,a)])
    best_action = argmax(Dict(
        a => uct(s,a) for a in ğ’œ_valid
    ))
    return best_action
end